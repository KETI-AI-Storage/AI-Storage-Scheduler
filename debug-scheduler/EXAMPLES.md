# AI Storage Scheduler - GPU Scheduling Examples

ê°„ë‹¨í•˜ê²Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ì˜ˆì‹œ íŒŒì¼ë“¤ì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ ëª©ë¡

### YAML ì˜ˆì‹œ íŒŒì¼
- **example-simple-gpu.yaml** - GPU 1ê°œ ìš”ì²­ Pod (ê°„ë‹¨í•œ busybox ì´ë¯¸ì§€)
- **example-cpu-only.yaml** - GPU ì—†ëŠ” CPU ì „ìš© Pod
- **example-deployment.yaml** - GPU Pod 2ê°œ ë³µì œë³¸ Deployment

### ìŠ¤í¬ë¦½íŠ¸
- **run-examples.sh** - ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ë°©ë²• 1: ëŒ€í™”í˜• ë©”ë‰´ ì‚¬ìš© (ì¶”ì²œ)
```bash
cd /root/workspace/ai-storage-scheduler/debug-scheduler
./run-examples.sh
```

ë©”ë‰´ê°€ ë‚˜íƒ€ë‚˜ë©´ ìˆ«ìë¥¼ ì„ íƒí•˜ì„¸ìš”:
- `1` - GPU Pod í…ŒìŠ¤íŠ¸
- `2` - CPU ì „ìš© Pod í…ŒìŠ¤íŠ¸
- `3` - GPU Deployment í…ŒìŠ¤íŠ¸
- `4` - ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- `s` - í˜„ì¬ ìƒíƒœ í™•ì¸
- `r` - í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ í™•ì¸
- `c` - ì •ë¦¬
- `q` - ì¢…ë£Œ

### ë°©ë²• 2: ì§ì ‘ ì‹¤í–‰
```bash
# GPU Pod í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
./run-examples.sh 1

# ë˜ëŠ”
./run-examples.sh simple-gpu

# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
./run-examples.sh all

# ìƒíƒœ í™•ì¸
./run-examples.sh status

# ì •ë¦¬
./run-examples.sh cleanup
```

### ë°©ë²• 3: kubectlë¡œ ì§ì ‘ ì‹¤í–‰
```bash
# 1. GPU Pod ìƒì„±
kubectl apply -f example-simple-gpu.yaml

# 2. Pod ìƒíƒœ í™•ì¸
kubectl get pod simple-gpu-pod -n keti -o wide

# 3. ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ í™•ì¸
kubectl logs -n keti -l app=ai-storage-scheduler --tail=20 | grep simple-gpu-pod

# 4. ì‚­ì œ
kubectl delete pod simple-gpu-pod -n keti
```

## ğŸ“ ì˜ˆì‹œ ìƒì„¸ ì„¤ëª…

### 1. example-simple-gpu.yaml
```yaml
# GPU 1ê°œë¥¼ ìš”ì²­í•˜ëŠ” ê°„ë‹¨í•œ Pod
# ê¸°ëŒ€ ê²°ê³¼: gpu-server-03 ë…¸ë“œì— ë°°ì¹˜
# ì‹¤í–‰ ì‹œê°„: 300ì´ˆ (5ë¶„)
```

**ì‚¬ìš©ë²•:**
```bash
kubectl apply -f example-simple-gpu.yaml
kubectl get pod simple-gpu-pod -n keti -o wide
```

**í™•ì¸ ì‚¬í•­:**
- Podê°€ `gpu-server-03` ë…¸ë“œì— ë°°ì¹˜ë˜ì—ˆëŠ”ê°€?
- ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ì— GPU ì •ë³´ê°€ í‘œì‹œë˜ëŠ”ê°€?

### 2. example-cpu-only.yaml
```yaml
# GPUë¥¼ ìš”ì²­í•˜ì§€ ì•ŠëŠ” CPU ì „ìš© Pod
# ê¸°ëŒ€ ê²°ê³¼: ëª¨ë“  ë…¸ë“œ ì¤‘ LeastAllocated ì ìˆ˜ê°€ ë†’ì€ ë…¸ë“œì— ë°°ì¹˜
# ì‹¤í–‰ ì‹œê°„: 300ì´ˆ (5ë¶„)
```

**ì‚¬ìš©ë²•:**
```bash
kubectl apply -f example-cpu-only.yaml
kubectl get pod simple-cpu-pod -n keti -o wide
```

**í™•ì¸ ì‚¬í•­:**
- Podê°€ ì–´ëŠ ë…¸ë“œì— ë°°ì¹˜ë˜ì—ˆëŠ”ê°€?
- ë¦¬ì†ŒìŠ¤ê°€ ê°€ì¥ ì—¬ìœ ë¡œìš´ ë…¸ë“œë¥¼ ì„ íƒí–ˆëŠ”ê°€?

### 3. example-deployment.yaml
```yaml
# GPU Pod 2ê°œ ë³µì œë³¸ì„ ìƒì„±í•˜ëŠ” Deployment
# ê¸°ëŒ€ ê²°ê³¼: ëª¨ë“  Podê°€ gpu-server-03ì— ë°°ì¹˜
# ì‹¤í–‰: ê³„ì† ì‹¤í–‰ (restartPolicy: Always)
```

**ì‚¬ìš©ë²•:**
```bash
kubectl apply -f example-deployment.yaml
kubectl get pods -n keti -l app=gpu-app -o wide
```

**í™•ì¸ ì‚¬í•­:**
- 2ê°œì˜ Podê°€ ëª¨ë‘ ìƒì„±ë˜ì—ˆëŠ”ê°€?
- ëª¨ë‘ `gpu-server-03`ì— ë°°ì¹˜ë˜ì—ˆëŠ”ê°€?
- gpu-server-03ì˜ GPUê°€ 2ê°œì´ë¯€ë¡œ ê°ê° 1ê°œì”© í• ë‹¹ë˜ì—ˆëŠ”ê°€?

## ğŸ” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: GPU Pod ìŠ¤ì¼€ì¤„ë§ ê²€ì¦
```bash
# 1. GPU Pod ìƒì„±
./run-examples.sh 1

# 2. ë°°ì¹˜ ë…¸ë“œ í™•ì¸
kubectl get pod simple-gpu-pod -n keti -o jsonpath='{.spec.nodeName}'
# ê¸°ëŒ€ê°’: gpu-server-03

# 3. ë…¸ë“œì˜ GPU ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get node gpu-server-03 -o jsonpath='{.status.allocatable.nvidia\.com/gpu}'
# ê¸°ëŒ€ê°’: 2

# 4. ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ í™•ì¸
kubectl logs -n keti -l app=ai-storage-scheduler | grep simple-gpu-pod
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë¦¬ì†ŒìŠ¤ ë¶„ì‚° ê²€ì¦
```bash
# 1. CPU Pod ì—¬ëŸ¬ ê°œ ìƒì„±
for i in {1..3}; do
  kubectl run cpu-pod-$i --image=busybox -n keti \
    --restart=Never --overrides='{"spec":{"schedulerName":"ai-storage-scheduler"}}' \
    -- sh -c "sleep 300"
done

# 2. Pod ë¶„ì‚° í™•ì¸
kubectl get pods -n keti -o wide | grep cpu-pod

# 3. ë…¸ë“œë³„ Pod ê°œìˆ˜ í™•ì¸
kubectl get pods -n keti -o wide --no-headers | awk '{print $7}' | sort | uniq -c

# 4. ì •ë¦¬
kubectl delete pod -n keti -l run=cpu-pod
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: GPU ë¶€ì¡± ìƒí™© í…ŒìŠ¤íŠ¸
```bash
# gpu-server-03ì€ GPU 2ê°œë§Œ ìˆìœ¼ë¯€ë¡œ, 3ê°œì˜ GPU Pod ìƒì„± ì‹œ 1ê°œëŠ” Pending

# 1. GPU Deployment ë³µì œë³¸ 3ê°œë¡œ ì„¤ì •
kubectl apply -f example-deployment.yaml
kubectl scale deployment gpu-deployment -n keti --replicas=3

# 2. Pod ìƒíƒœ í™•ì¸
kubectl get pods -n keti -l app=gpu-app -o wide

# 3. Pending Pod í™•ì¸
kubectl get pods -n keti -l app=gpu-app --field-selector=status.phase=Pending

# 4. Pending ì‚¬ìœ  í™•ì¸
kubectl describe pod <pending-pod-name> -n keti | grep "Insufficient"

# 5. ì •ë¦¬
kubectl delete deployment gpu-deployment -n keti
```

## ğŸ› ï¸ ë””ë²„ê¹… íŒ

### ë¡œê·¸ í™•ì¸
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
kubectl logs -n keti -l app=ai-storage-scheduler -f

# íŠ¹ì • Pod ê´€ë ¨ ë¡œê·¸ë§Œ
kubectl logs -n keti -l app=ai-storage-scheduler | grep "pod=simple-gpu-pod"

# GPU ê´€ë ¨ ë¡œê·¸ë§Œ
kubectl logs -n keti -l app=ai-storage-scheduler | grep "gpu"

# ìµœê·¼ 50ì¤„
kubectl logs -n keti -l app=ai-storage-scheduler --tail=50
```

### Pod ìƒíƒœ í™•ì¸
```bash
# Pod ì´ë²¤íŠ¸ í™•ì¸
kubectl describe pod simple-gpu-pod -n keti

# Pod YAML í™•ì¸
kubectl get pod simple-gpu-pod -n keti -o yaml

# Podì˜ ë¦¬ì†ŒìŠ¤ ìš”ì²­ í™•ì¸
kubectl get pod simple-gpu-pod -n keti -o jsonpath='{.spec.containers[0].resources}'
```

### ë…¸ë“œ ìƒíƒœ í™•ì¸
```bash
# ì „ì²´ ë…¸ë“œ ë¦¬ì†ŒìŠ¤
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.allocatable.cpu,MEMORY:.status.allocatable.memory,GPU:.status.allocatable."nvidia\.com/gpu"

# íŠ¹ì • ë…¸ë“œ ìƒì„¸ ì •ë³´
kubectl describe node gpu-server-03

# GPU í• ë‹¹ í˜„í™©
kubectl describe node gpu-server-03 | grep -A 10 "Allocated resources"
```

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

### ì •ìƒ ë™ì‘ ì‹œ
```
NAME              READY   STATUS    NODE            GPU
simple-gpu-pod    1/1     Running   gpu-server-03   1
simple-cpu-pod    1/1     Running   csd-server-01   0
gpu-deployment-*  1/1     Running   gpu-server-03   1
gpu-deployment-*  1/1     Running   gpu-server-03   1
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸
```
level=INFO msg="[event] add pod to scheduling queue" namespace=keti pod=simple-gpu-pod gpu=1
level=INFO msg="[gpu-metrics] Fetching GPU metrics for node" node=gpu-server-03
```

## ğŸ§¹ ì •ë¦¬

### ê°œë³„ ì‚­ì œ
```bash
kubectl delete pod simple-gpu-pod -n keti
kubectl delete pod simple-cpu-pod -n keti
kubectl delete deployment gpu-deployment -n keti
```

### ì¼ê´„ ì •ë¦¬
```bash
./run-examples.sh cleanup
```

ë˜ëŠ”
```bash
kubectl delete pods,deployments -n keti -l 'app in (gpu-app)'
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **GPU ë…¸ë“œ**: í˜„ì¬ `gpu-server-03`ë§Œ GPU 2ê°œ ë³´ìœ 
2. **ë™ì‹œ ì‹¤í–‰**: GPU Pod 2ê°œê¹Œì§€ë§Œ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥
3. **ìŠ¤ì¼€ì¤„ëŸ¬ ì´ë¦„**: ëª¨ë“  ì˜ˆì‹œëŠ” `schedulerName: ai-storage-scheduler` ì‚¬ìš©
4. **ë„¤ì„ìŠ¤í˜ì´ìŠ¤**: ëª¨ë“  ë¦¬ì†ŒìŠ¤ëŠ” `keti` ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìƒì„±

## ğŸ“š ì¶”ê°€ ì°¸ê³ 

- **ì „ì²´ í…ŒìŠ¤íŠ¸**: `./test.sh` ì‚¬ìš© (ìë™í™”ëœ ì „ì²´ í…ŒìŠ¤íŠ¸)
- **README.md**: ì „ì²´ ë””ë²„ê¹… ê°€ì´ë“œ
- **ìƒì„¸ ë¬¸ì„œ**: `/root/workspace/ai-storage-scheduler/CLAUDE.md`
